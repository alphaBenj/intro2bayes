{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Statistics\n",
    "- Alternative statistics to what is thought in most schools\n",
    "- Became more usefull with recent advances in math and computation\n",
    "- Works well when data is limited\n",
    "- Applications vary from simple models to state of art machine learning, rocket science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Formal Definition\n",
    "$$P(\\theta\\mid X) = \\frac{P(X\\mid\\theta)P(\\theta)}{P(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "$$P(\\theta)$$\n",
    "----\n",
    " - Our confidence in a certain theory before we consider new data\n",
    " -  $\\theta$  could stand for specific probability, one or more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probaility\n",
    "$$P(X \\mid \\theta)$$\n",
    "----\n",
    " - Also know as Likelihood\n",
    " - Probability of seeing data $X$ under assumption that there is a 100% chance that theory $\\theta$ is true\n",
    " - Sometimes presented with multiple assumptions $P(X \\mid \\mu,\\sigma)$\n",
    " - Sometimes presented with multiple datapoints $P(X, Y \\mid \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Probability\n",
    "$$P(X)$$\n",
    "----\n",
    " - Also known as Evidence, Normalizing constant\n",
    " - Probaility of seeing $X$ data without assuming that any specific theory is true\n",
    " - Sum of products of all $\\theta$\n",
    " - Sometimes excluded from the equaltion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "$$P(\\theta \\mid X)$$\n",
    "----\n",
    " - Our confidence in certain theory **after** we consider new data\n",
    " - Updated version of $P(\\theta)$\n",
    " - If we get more data in the future this becomes our new posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule Intuition\n",
    "\n",
    " - Today we get of see only who tested positive on a medical exam\n",
    " - We know that people who have cancer are more likely to test positive on exam\n",
    "<img src=\"../asset/disease venn.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookie Problem (from Think Bayes) cont.\n",
    "#### Marginal Probability calculation\n",
    "\n",
    "$ P(Chocolate) = P(Bowl1) \\times P(Chocolate \\mid Bowl1) + P(Bowl2) \\times P(Chocolate \\mid Bowl2)$ \n",
    "\n",
    "$ 62.5\\% = 50\\% \\times 50\\% + 50\\% \\times 25\\% $ \n",
    "\n",
    "$ P(Vanilla) = P(Bowl1) \\times P(Vanilla \\mid Bowl1) + P(Bowl2) \\times P(Vanilla \\mid Bowl2)$\n",
    "\n",
    "$ 37.5\\% = 50\\% \\times 50\\% + 50\\% \\times 75\\% $ \n",
    "\n",
    "#### Posterior calculation\n",
    "\n",
    "$ P(Bowl1 \\mid Chocolate) = \\frac{P(Chocolate \\mid Bowl1) \\times P(Bowl1)}{P(Chocolate)} $\n",
    "\n",
    "$ 40\\% = \\frac{50\\% \\times 50\\%}{62.5\\%} $\n",
    "\n",
    "$ P(Bowl2 \\mid Chocolate) = \\frac{P(Chocolate \\mid Bowl2) \\times P(Bowl2)}{P(Chocolate)} $\n",
    "\n",
    "$ 60\\% = \\frac{75\\% \\times 50\\%}{62.5\\%} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Regession\n",
    " - Regression model just like normal linear regession\n",
    " - Predictions and coefficients are not point estimates they are probability distributions\n",
    " - If you have a reasonable assumptions around its coeficients/priors it will be more accurate than traditional Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notable Bayesian Models\n",
    "- Naive Bayes\n",
    "- Bayesian Network \n",
    "- Bayesian Deep Neural Network\n",
    "- Kalman Filter\n",
    "- Bayesian Structural Time Series\n",
    "- Bayesian Model Averaging Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notable Models: Naive Bayes\n",
    "- Simple model\n",
    "- Scalable\n",
    "- Frequently used as a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Network\n",
    "- Describes conditional probabilities\n",
    "\n",
    "<img src=\"../asset/bayesian network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Deep Neural Network\n",
    "- Full power of neural network\n",
    "- if data is scarce performs better than regular neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kalman Filter\n",
    " - __powerfull__ model for estimation\n",
    " - Improves estimates of not trustworthy models\n",
    " - Does not trust bad(noisy) data\n",
    " - Keeps on updating estimates even if we don't have new data\n",
    " - Trusted by NASA in putting a man on the moon (Apollo 11 guidance computer)\n",
    " <img src=\"../asset/kalman nasa.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Strutural Time Series\n",
    " - __Kalman Filter__ that doesn't just improve estimates it updates the model\n",
    " - Estimates with multiple versions of the model simultaniously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Model Averaging Ensambles\n",
    " - Meta-model that leverages error estimates of multiple bayesian models\n",
    " - Per Tom Mitchell this will always be the best enambling technique\n",
    " \n",
    " Tom M. Mitchell, Machine Learning, 1997, pp. 175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Steps\n",
    "Free resources:\n",
    " - Think Bayes by Allen Downey (link)[http://greenteapress.com/wp/think-bayes/]\n",
    " - Bayesian Methods for Hackers (link)[https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers]\n",
    " - Introduction to Bayesian data analysis by Rasmus Bååth (link)[https://www.youtube.com/watch?v=3OJEae7Qb_o]\n",
    "\n",
    "Concepts to learn/review:\n",
    "- Go back to fundamentals(Probability theory)\n",
    "- More types of probability distributions, Beta, Gamma, Possion, Multivariate Gagussian. etc.\n",
    "- Learn programming in Stan, PyMc3"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
